# Отчет по лабораторной работе: "Масштабирование приложений"

## Выполненные задания
Все скриншоты находятся в папке *screenshots* этого репозитория

### Задание 1-3: Docker Registry
**Задание 2**: Запущен insecure Docker registry с внешним хранилищем. Продемонстрирована работа с локальным registry: образ `hello-world` успешно загружен и выгружен. 

*Скриншоты "Задание 2"*

**Задание 3**: Настроен secure Docker registry с аутентификацией. Реализовано:
- Генерация SSL сертификатов
- Настройка базовой аутентификации через htpasswd
- Успешный push/pull образов с аутентификацией

*Результаты подтверждены скриншотами успешной и неудачной аутентификации. (Скриншоты "Задание 3")*

### Задание 4: Docker Orchestration Hands-on Lab
**Выполнено**:
- Инициализация Docker Swarm (single-node cluster)
- Создание сервиса `sleep-app` и масштабирование от 1 до 7 реплик
- Перевод узла в состояние Drain и обратно в Active

*Скриншоты "Задание 4"*

**Ответы на вопросы**:
1. *Восстановилась ли работа запущенного сервиса на узле после возврата в Active?*  
   Нет, при переводе узла из Drain обратно в Active сервисы не восстанавливаются автоматически. Swarm не переносит задачи обратно на узел — необходимо явно масштабировать сервис заново.

2. *Что необходимо сделать для запуска службы на узле снова?*  
   После возврата узла в Active нужно заново масштабировать сервис:  
   ```bash
   docker service scale <service-name>=<replicas>
   ```
   Swarm начнет размещать новые задачи на активном узле при обновлении или масштабировании сервиса.

### Задание 5: Swarm Stack Introduction
**Выполнено**:
- Развертывание voting-app как stack в Swarm
- Проверка работы всех 6 сервисов stack'а

*Скриншоты "Задание 5"*

**Ответы на вопросы**:
1. *Как конфигурируется количество нод в стеке?*  
   Количество реплик указывается в секции `deploy:` для каждого сервиса в файле docker-stack.yml:
   ```yaml
   deploy:
     replicas: 2
   ```

2. *Как организуется проверка жизнеспособности сервисов?*  
   Через healthcheck-опции в Docker Compose:
   ```yaml
   healthcheck:
     test: ["CMD", "curl", "-f", "http://localhost"]
     interval: 30s
     timeout: 10s
     retries: 3
   ```

---

## Задание 6: Масштабирование приложения-счетчика
**Задания по раобте с приложением счетчика https://github.com/nzhukov/counter-deploy**

### 6.1 Кластеризация с 4 инстансами Flask

#### Конфигурация
Файл: `docker-stack.yml`
- 4 реплики сервиса `app` (Flask + Gunicorn)
- 1 реплика Redis

#### Результаты нагрузочного тестирования
**Методология**: 100 GET-запросов к `/api/counter` и 10 POST-запросов к `/api/counter/increment`

**Результаты**:
- **GET RPS**: 188.43 запросов в секунду
- **POST RPS**: 68.79 запросов в секунду
- **Общее время**: 0.68 секунд

**Вывод**: При увеличении количества реплик Flask с 1 до 4 производительность обработки GET-запросов значительно увеличивается, так как нагрузка распределяется между несколькими инстансами. POST-запросы показывают меньший прирост из-за необходимости синхронизации с Redis.

### 6.2 Увеличение количества инстансов Redis

#### Проблемы при репликации Redis
После масштабирования Redis до 2 реплик:
```bash
docker service scale counter_redis=2
```

**Получены результаты**:
- GET RPS: 221.98 (рост на 18%)
- POST RPS: 209.18 (рост на 204%)

**Но это иллюзия производительности**, произошло следующее:

1. **Отсутствие репликации данных**: Каждый инстанс Redis имеет свою собственную базу
2. **Нарушение консистентности**: 
   - Инкремент счетчика может попасть в Redis 1
   - Следующий запрос на чтение может попасть в Redis 2 и получить старое значение
   - Счетчик "размножается" по разным базам
3. **Ложный рост производительности**: Увеличение RPS связано с распределением запросов между двумя серверами, но данные становятся несогласованными

#### Особенности реплицированного сервиса с БД
**Проблемы**:
1. Консистентность данных при чтении/записи
2. Автоматический failover при отказе master-узла
3. Синхронизация данных между репликами
4. Распределение нагрузки между read-only репликами

**Пути решения**:
1. **Redis Sentinel** - для автоматической репликации master-slave
2. **Redis Cluster** - для горизонтального масштабирования с шардированием
3. **Read replicas** - использовать один master для записи, несколько реплик для чтения

**Минимальное решение для Docker Swarm**:
```yaml
redis:
  image: redis:7-alpine
  command: redis-server --appendonly yes --replicaof redis-master 6379
  deploy:
    replicas: 3
```

### 6.3 Конфигурация для Kubernetes

Пример конфигурации в файле `k8s-deployment.yaml`:

**Основные отличия от Swarm**:
1. Более сложная структура (отдельные Deployment, Service, ConfigMap объекты)
2. Явное указание ресурсов (CPU, memory limits/requests)
3. Необходимость настройки Service для сетевого доступа
4. Более гибкие политики обновления (rolling updates, blue-green)
5. Наличие readiness/liveness проб для проверки здоровья
6. Поддержка StatefulSets для stateful-приложений

**Преимущества Kubernetes для данного приложения**:
- Лучшая поддержка stateful-приложений (StatefulSets для Redis)
- Автоматическое перебалансирование нагрузки
- Встроенные механизмы для канареечных развертываний
- Более зрелая экосистема (Helm, Operators)

### 6.4 Репозиторий с отчетом

**Структура репозитория**:
```
DockerCounter/
├── backend/                    # Flask приложение
│   ├── app.py                 # Основное приложение
│   └── requirements.txt       # Зависимости Python
├── frontend/                  # Vue.js фронтенд
├── docker-stack.yml           # Конфигурация для Docker Swarm
├── k8s-deployment.yaml       # Конфигурация для Kubernetes
├── Dockerfile                # Многостадийная сборка
├── load-test.sh              # Скрипт нагрузочного тестирования
└── README.md                 # Данный отчет
```

**Ключевые моменты конфигурации**:

1. **docker-stack.yml** - использует директиву `deploy.replicas` для управления количеством инстансов
2. **Масштабирование Flask**: 4 реплики обеспечивают распределение нагрузки
3. **Проблема с Redis**: прямое масштабирование без репликации нарушает консистентность
4. **Health checks**: в Kubernetes добавлены readiness/liveness пробы

## Выводы

1. **Stateless vs Stateful**: Stateless-сервисы (Flask) легко масштабируются горизонтально, в то время как stateful-сервисы (Redis) требуют специальных подходов к репликации и синхронизации.

2. **Docker Swarm**: Хорошее решение для быстрого развертывания простых кластеризованных приложений, но имеет ограниченные возможности для сложных stateful-приложений.

3. **Kubernetes**: Предоставляет более мощные инструменты для оркестрации, особенно для stateful-приложений, но требует больше времени на изучение и настройку.

4. **Производительность**: Горизонтальное масштабирование stateless-компонентов действительно увеличивает потенциал обработки запросов, но становится узким местом при работе с разделяемым состоянием (базами данных).

5. **Рекомендации**: Для продакшн-среды необходимо:
   - Использовать Redis Cluster или управляемый Redis-сервис
   - Реализовать стратегию репликации базы данных
   - Настроить health checks и мониторинг
   - Продумать стратегию балансировки нагрузки между stateless-компонентами

---

**Технические детали реализации доступны в соответствующих файлах конфигурации. Все компоненты приложения успешно развертываются как в Docker Swarm, так и могут быть адаптированы для Kubernetes.**